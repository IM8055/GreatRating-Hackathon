{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HackathonBook__8602.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JBivu05F9Vjd"},"source":["# G-Drive"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WQ2hQkCO75zj","outputId":"2b106552-a4ea-48ca-e824-3e259e640d62","executionInfo":{"status":"ok","timestamp":1589131737762,"user_tz":-330,"elapsed":1441,"user":{"displayName":"Kavin Annugala Karunakaran","photoUrl":"","userId":"05343623427432865419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# # # MOUNTING DRIVE # # #\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UKwsybst9MyM"},"source":["# Dependancies"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9CW_naj-8-kI","outputId":"76e39f01-1cc6-42a1-9388-76f01051a189","executionInfo":{"status":"ok","timestamp":1589131741467,"user_tz":-330,"elapsed":5097,"user":{"displayName":"Kavin Annugala Karunakaran","photoUrl":"","userId":"05343623427432865419"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["import os\n","import pandas as pd\n","import pandas as pd\n","import numpy as np\n","import re\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.pyplot import figure\n","plt.style.use('ggplot')\n","plt.rcParams['figure.figsize'] = (12,8)\n","\n","import datetime\n","import time\n","\n","from scipy import stats\n","\n","\n","import xgboost as xgb\n","from xgboost.sklearn import XGBClassifier\n","\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","import sklearn.metrics as metrics\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import average_precision_score,make_scorer\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.neighbors import KNeighborsClassifier \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.utils import class_weight\n","from sklearn import preprocessing\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","from nltk.classify.scikitlearn import SklearnClassifier\n","\n","from imblearn.pipeline import Pipeline\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn import svm\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier \n","from sklearn.decomposition import PCA\n","\n","from imblearn.over_sampling import SMOTE\n","\n","from mlxtend.classifier import StackingClassifier\n","\n","from collections import Counter\n","\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation\n","import tensorflow as tf\n","from keras.optimizers import sgd\n","\n","import pickle"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n","/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n","Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TweXvIOT9det"},"source":["# Variables"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6sczxFvz9I_N","colab":{}},"source":["fPath = '/content/gdrive/My Drive/AIML/Hackathon_04112020/wd/Files/'\n","\n","# # # File Name # # # \n","bond_ratings = 'bond_ratings.csv'\n","fund_allocations = 'fund_allocations.csv'\n","fund_config = 'fund_config.csv'\n","fund_ratios = 'fund_ratios.csv'\n","fund_specs = 'fund_specs.csv'\n","other_specs = 'other_specs.csv'\n","return_10year = 'return_10year.csv'\n","return_3year = 'return_3year.csv'\n","return_5year = 'return_5year.csv'\n","\n","# # # Variables # # #\n","baseYear = 2020"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"L-5RCaIx_Ay7"},"source":["# Loading files"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hCkuFnj9_BhT","colab":{}},"source":["dfBondRatings = pd.read_csv(fPath+bond_ratings)\n","dfFundAllocations = pd.read_csv(fPath+fund_allocations)\n","dfFundConfig = pd.read_csv(fPath+fund_config)\n","dfFundRatios = pd.read_csv(fPath+fund_ratios, thousands = \",\")\n","dfFundSpecs = pd.read_csv(fPath+fund_specs)\n","dfOtherSpecs = pd.read_csv(fPath+other_specs, thousands = \",\")\n","dfReturn10year = pd.read_csv(fPath+return_10year, thousands = \",\")\n","dfReturn3year = pd.read_csv(fPath+return_3year, thousands = \",\")\n","dfReturn5year = pd.read_csv(fPath+return_5year, thousands = \",\")\n","\n","dfSample = pd.read_csv(fPath+'sample_submission.csv') # Sample Submission\n","\n","# # # Datframes Dictionary # # #\n","dictDataFrames = {'dfBondRatings':dfBondRatings,\n","                  'dfFundAllocations':dfFundAllocations,\n","                  'dfFundConfig':dfFundConfig,\n","                  'dfFundRatios':dfFundRatios,\n","                  'dfFundSpecs':dfFundSpecs,\n","                  'dfOtherSpecs':dfOtherSpecs,\n","                  'dfReturn10year':dfReturn10year,\n","                  'dfReturn3year':dfReturn3year,\n","                  'dfReturn5year':dfReturn5year}\n","\n","dfOtherSpecs.rename(columns = {'2012_fund_return': '2012_return_fund', \n","                                  '2013_category_return':'2013_return_category', \n","                                  '2014_category_return': '2014_return_category',\n","                                  '2017_category_return' :'2017_return_category',\n","                                  'category_return_2015' : '2015_return_category',\n","                                  '1_month_fund_return' : 'fund_return_1months',\n","                                  '1_year_return_fund' : 'fund_return_1years',\n","                                  '3_months_return_category' : 'category_return_3month',\n","                                  'cash_percent_of_portfolio':'cash_percentage_of_portfolio',\n","                                  'stock_percent_of_portfolio' : 'stock_percentage_of_portfolio'}, inplace = True) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OC_lYRgDHGbW"},"source":["# Functions\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FV7UYSXTHKrg"},"source":["## DataFrame Functions\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zp-ZWgJMGQxU","colab":{}},"source":["def funcValueCount(dfName,colName = ''):\n","  '''\n","    Displays the value counts of an specified dataframe and coloumn present in the dataframe.\n","\n","  '''\n","  if dfName.__class__ == dict:\n","    for key,df in dfName.items():\n","      if key != 'dfReturn10year':\n","        print('\\n')\n","        print('----- Dataframe {}: -----'.format(key))\n","        vc = df[colName].value_counts(dropna = False)\n","        print('Duplicate values in {} column are {}\\n'.format(colName,vc[vc > 1]))\n","  else:\n","    vc = dfName[colName].value_counts(dropna = False)\n","    print('Duplicate values in {} column are {}\\n'.format(colName,vc[vc > 1]))\n","\n","def funcNullValueColumns(dfName):\n","  '''\n","  Prints the columns that have NaN values in them\n","  ''' \n","  [col for col in pd.DataFrame(dfName).columns if pd.DataFrame(dfName)[col].isnull().any()]\n","\n","def funcIsNull(dfName, colwise = True,threshold = 50, dropColumn = False):\n","    '''\n","    * Prints the NaN percentage in each column of the dataframe that is greater than the threshold.\n","    * Returns the list of column names whose percentage is greater than than the threshold.\n","    * if dropColumn is True drops the columns whose NaN percentage is high. \n","\n","    '''\n","    if dfName.__class__ == dict:\n","      for key,value in dfName.items():\n","        # print('\\n----------Columns whose NaN percentage is greater than or equal to {} in {} dataframe\\n'.format(threshold,key))\n","        series = value.isna().mean().round(4) * 100\n","        for identity,col in enumerate(series):\n","          if col > threshold:\n","            print('{}. {} --------- {}'.format(identity+1,series.index[identity],round(col,2)))\n","    else:\n","      if colwise == True:\n","        count = 1\n","        highNanCol = []\n","        dfNew = pd.DataFrame()\n","        series = dfName.isna().mean().round(4) * 100\n","        # print('Columns whose NaN percentage is greater than or equal to {}\\n'.format(threshold))\n","        for identity,col in enumerate(series):\n","          if col >= threshold:\n","            # print('{}. {} --------- {}%'.format(count,series.index[identity],round(col,2)))\n","            count = count + 1\n","            highNanCol.append(series.index[identity])\n","        if dropColumn == True and len(highNanCol) >= 1:\n","          dfNew = dfName.drop(columns = highNanCol) \n","          return dfNew\n","        else:\n","          return highNanCol\n","      else:\n","        highNanRow = []\n","        # print('Rows whose NaN percentage is greater than or equal to {}\\n'.format(threshold))\n","        for i in range(len(dfName.index)) :\n","            temp = round((dfName.iloc[i].isnull().sum()/dfName.shape[1])*100,2)\n","            if temp >= threshold:\n","              highNanRow.append(dfName.iloc[i]['fund_id'])\n","        return highNanRow\n","\n","def funcColName(dfName = dictDataFrames):\n","  '''\n","  Prints the column names in a dataframe.\n","  \n","  '''\n","  colName = []\n","  totCol = 1\n","  duplicateCol = []\n","  if dfName.__class__ == dict:\n","    for key,value in dfName.items():\n","      print('\\n')\n","      print('----- The Columns in {} dataframe are: -----'.format(key))\n","      for count, col in enumerate(value.columns): \n","        print('{}. {}'.format(count+1, col))\n","        totCol = totCol + 1\n","    print('************ Total Columns are {} ************'.format(totCol))\n","  else:\n","    for count,col in enumerate(dfName.columns.sort_values()):\n","      colName.append(col)\n","    return colName\n","\n","\n","def funcRenameColumn(dfName = dictDataFrames, newColName = 'uniqueID'):\n","  '''\n","  Renames the columns in all the dataframes and changes the Renamed column datatype to 'category'. Takes in a dictionary as an input.\n","\n","  '''\n","  if dfName.__class__ == dict:\n","    for key,df in dfName.items():\n","      if 'tag' in df.columns:\n","        df.rename(columns={'tag':newColName}, inplace = True)\n","        df[newColName]=df[newColName].astype('category')\n","      elif 'id' in df.columns:\n","        df.rename(columns={'id':newColName}, inplace = True)\n","        df[newColName]=df[newColName].astype('category')\n","\n","def funcDataFrameNames(dfName = dictDataFrames):\n","  '''\n","  Lists the names in the dataframe dictionary\n","  '''\n","  slNo = 1\n","  if dfName.__class__ == dict:\n","    for key,value in dfName.items():\n","      print('{}. {}'.format(slNo,key))\n","      slNo = slNo + 1\n","\n","def funcConcatDF(dfName = dictDataFrames, uniqueID = 'uniqueID'):\n","  '''\n","  Concatenates the dataframesa and creates teh Master dataframe\n","  Removes duplicate columns.\n","  Renames the columns.\n","  '''\n","  newDF = pd.DataFrame()\n","  newDF_ = pd.DataFrame()\n","  listDuplicateCol = []\n","  listFundID = ['dfReturn10year','dfFundConfig','dfFundRatios']\n","  # DataFrames that have a 'fund_id' column\n","  if dfName.__class__ == dict:\n","    for key,df in dfName.items():\n","      if newDF.empty == True and key not in listFundID:\n","        newDF = df\n","      elif newDF.empty == False and key not in listFundID:\n","        newDF = pd.merge(newDF, df, on = uniqueID, how='inner')\n","      elif newDF_.empty == True and key in listFundID:\n","        newDF_ = df\n","      elif newDF_.empty == False and key in listFundID:\n","        newDF_ = pd.merge(newDF_, df, on = 'fund_id', how='inner')\n","  newDF = pd.merge(newDF, newDF_, on = 'uniqueID', how='inner')\n","  # Removes the duplicate columns \n","  for colName in newDF.columns:\n","    if colName[-2:] == '_x':\n","      listDuplicateCol.append(colName)\n","  # Removes the uniqueID column by appending to the list listDuplicateCol\n","  listDuplicateCol.append('uniqueID')\n","  newDF = newDF.drop(columns = listDuplicateCol)\n","  # Renames the column by stripping _y at the end\n","  newDF.rename(columns = lambda x : str(x)[:-2] if x[-2:] == '_y' else x, inplace = True)\n","  del newDF_\n","  return newDF\n","\n","def funcTestSetSplit(dfName):\n","  '''\n","  This function splits the actual test data from the combined dataframes\n","  '''\n","  dfActualTrain = dfName[dfName['greatstone_rating'].notna()]\n","  dfActualTest = dfName[dfName['greatstone_rating'].isnull()]\n","  return  dfActualTrain, dfActualTest\n","\n","  #-----------------#-----------------#\n","\n","def funcColStringSearch(dfName,word, lastWordSearch = 'any'):\n","  '''\n","  If lastWordSearch is 'last' it returns the column names that end with the 'word' hyperparameter else searches 'word' in the whole column\n","  The last argument only takes any, last, first\n","  '''\n","  listColName = ['fund_id']\n","  if lastWordSearch == 'any':\n","    for colName in dfName.columns:\n","      if word in colName:\n","        listColName.append(colName)\n","  elif lastWordSearch == 'last':\n","    searchwordLen = len(word) * -1\n","    for colName in dfName.columns:\n","      if colName[searchwordLen:] == word:\n","        listColName.append(colName)\n","  elif lastWordSearch == 'first':\n","    searchwordLen = len(word)\n","    for colName in dfName.columns:\n","      if colName[:searchwordLen] == word:\n","        listColName.append(colName)\n","  else:\n","    print(\"Wrong last argument in funcColStringSearch\")\n","  return listColName\n","\n","  #-----------------#-----------------#\n","\n","def funcChangeColDataType(dfname, listColumn, type):\n","  '''\n","    Changes the datatype of the column\n","  '''\n","  for colName in dfName.columns:\n","    if dfName[colName] != np.float64:\n","      dfName[colName].astype('float64')\n","\n","def funcColSearch(colName, dfName = dictDataFrames):\n","  '''\n","  Prints the dataframes that have the queried column\n","  '''\n","   for key,value in dfName.items():\n","     if colName in value.columns:\n","       print(key)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IIY6V46S-96c"},"source":["## ML Functions"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4a1QC-fuxC--"},"source":["### funcTopXOneHotEncode"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X-2bSvMv--XP","colab":{}},"source":["def funcTopXOneHotEncode(dfName, feature, top = 20, dropOriginal = False, trainSet = True, sortAscending = False,listFeatureCategories = []):\n","  '''\n","  One hot encodes top or bottom X most frequent columns alone\n","  '''\n","  if trainSet == True:\n","    listTopXCategories = []\n","    if len(dfName[feature].unique()) >= top:\n","      listTopXCategories = dfName[feature].value_counts().sort_values(ascending = sortAscending).head(top).index\n","    else:\n","      listTopXCategories = dfName[feature].value_counts().sort_values(ascending = sortAscending).index\n","    for label in listTopXCategories:\n","      dfName[feature+'_'+label] = np.where(dfName[feature] == label,1,0)\n","    if dropOriginal == True:\n","      dfName = dfName.drop(columns = feature)\n","    return dfName, listTopXCategories\n","  else:\n","    for label in listFeatureCategories:\n","      dfName[feature+'_'+label] = np.where(dfName[feature] == label,1,0)\n","    if dropOriginal == True:\n","      dfName = dfName.drop(columns = feature)\n","    return dfName"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x937Vzj_e747"},"source":["### funcCustomCrossValidate()"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"leCs12ACb7xg","colab":{}},"source":["def funcCustomCrossValidate(dfName, modelName, folds = 5):\n","  '''\n","  Custom K-folds implementation\n","  '''\n","  kf = KFold(n_splits = folds, shuffle = True)\n","  scores = []\n","  for i in range(folds):\n","      print('Split', folds)\n","      result = next(kf.split(dfName), None)\n","      x_train = dfName.iloc[result[0]]\n","      x_test = dfName.iloc[result[1]]\n","      y_train = y.iloc[result[0]]\n","      y_test = y.iloc[result[1]]\n","      model = modelName.fit(x_train,y_train)\n","      predictions = rf_reg.predict(x_test)\n","      scores.append(metrics.precision_score(np.array(y_test['greatstone_rating']), predictions, average = None).mean())\n","  return model, scores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S6582WUEMfjb"},"source":["### Model Functions\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9RaHBm_FMm3A","colab":{}},"source":["def funcCreateBalancedSampleWeights(y_train, largest_class_weight_coef):\n","  '''\n","  Create a list of weights associated with every row. It is used in certain ML models for penalizing imbalnce in the dataset.\n","  largest_class_weight_coef -> is the probability of the largest occuring class\n","  '''\n","    classes = y_train.unique()\n","    classes.sort()\n","    class_samples = np.bincount(y_train)\n","    total_samples = class_samples.sum()\n","    n_classes = len(class_samples)\n","    weights = total_samples / (n_classes * class_samples * 1.0)\n","    class_weight_dict = {key : value for (key, value) in zip(classes, weights)}\n","    class_weight_dict[classes[1]] = class_weight_dict[classes[1]] * largest_class_weight_coef\n","    sample_weights = [class_weight_dict[y] for y in y_train]\n","    return sample_weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PNqYfSlgwfpC"},"source":["## Fast Functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"THDtCV5MwgH5","colab":{}},"source":["def funcListItemRemovals(listOriginal, listRemoveItems = []):\n","  '''\n","  Removes the items in the list\n","  '''\n","  for item in listRemoveItems:\n","    if item in listOriginal:\n","      listOriginal.remove(item)\n","    else:\n","      print(item, ' not available in list')\n","  return listOriginal"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sJi4uQ_fI85U"},"source":["## Feature Engineering"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"G4_5K3UFUeGi"},"source":["### Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"lsxU-1vs_ial","colab_type":"text"},"source":["#### Feature Support Functions"]},{"cell_type":"code","metadata":{"id":"u-gUmiaO_i3N","colab_type":"code","colab":{}},"source":["def funcYearFeatureEngineering(dfNameSearch, df, trainFuncSet1,year = ''):\n","  '''\n","  This function help us to feature engineer the columns present in dfReturn10year, dfReturn5year, dfReturn3year.\n","  * Calculates the difference of Fund Mean catrgorywise and the fund value.\n","  * Calculates the difference between Fund Value and Category value.\n","  '''\n","  listSearch = ['alpha','beta','annual','r_squared', 'std', 'sharpe', 'treynor']\n","  listCategory = np.unique(np.array(trainFuncSet1['category'])).tolist()\n","  for name in listSearch:\n","    listTemp = funcColStringSearch(dfNameSearch,name)\n","    listTemp.remove('fund_id')\n","    for colNames in listTemp:\n","      if 'fund' in colNames:\n","        fund = colNames\n","      else:\n","        category = colNames\n","    tempName = year + name + '_YearReturn_FeatureEngineering'\n","    df[tempName] = df[fund] - df[category]\n","    tempName = year + name + '_FundYearReturn_FeatureEngineering'\n","    df[tempName] = df[fund] - trainFuncSet1[fund].mean(skipna = True)\n","\n","    for colName in listTemp:\n","      if '_category' not in colName:\n","        tempName = colName + '_FundCategoryReturn_FeatureEngineering'\n","        df[tempName] = 0\n","        for category in listCategory:\n","          categoryMean = trainFuncSet1.loc[trainFuncSet1['category'] == category,[colName]].mean(skipna = True)\n","          df.loc[df['category']==category, tempName] =   df.loc[df['category']==category, colName] - categoryMean[0]\n","        df[tempName] = df[tempName].fillna(value= 0.00000001)\n","\n","  return df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ry937tKpRtd4"},"source":["#### Engineering 1"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"m_I21QAzI9LQ","colab":{}},"source":["def funcFeatureEngineering(dfName):\n","  '''\n","  Feature Engineering function 1\n","  '''\n","\n","  # # # Treating Inception Date # # # \n","  if 'inception_date' in dfName.columns and 'Days_Since_Inception' not in dfName.columns :\n","    dfName['inception_date'] = pd.to_datetime(dfName['inception_date'])\n","    dfName['Days_Since_Inception'] = (datetime.datetime.now()  - dfName['inception_date']).dt.days\n","\n","  dfName['AverageAssets_FeatureEngineering'] = dfName['total_assets'] / (2020 - dfName['inception_date'].dt.year)\n","  dfName['AverageAssets_FeatureEngineering'] = dfName['AverageAssets_FeatureEngineering'].fillna(value= -999)\n","  dfName['yeild_expense_ratio'] = dfName['yield'] / dfName['fund_ratio_net_annual_expense']\n","  dfName['yeild_expense_ratio'] = dfName['yeild_expense_ratio'].fillna(value= -999)\n","  \n","  # # # Beta Feature Enginerring # # #\n","  listTempColName = funcColStringSearch(dfName, 'beta')\n","  listTempColName.remove('fund_id')\n","  for col in listTempColName:\n","    nameTemp = col + '_beta_FeatureEngineer'\n","    dfName[nameTemp]=-1\n","    dfName.loc[dfName[col]<1,nameTemp] = 0\n","    dfName.loc[dfName[col] == 1,nameTemp] = 1\n","    dfName.loc[dfName[col] > 1,nameTemp] = 2\n","\n","  # # # Treating R Squared # # #\n","  listTempColName = funcColStringSearch(dfName, 'r_squared')\n","  listTempColName.remove('fund_id')\n","  for col in listTempColName:\n","    nameTemp = col + '_rSquare_FeatureEngineer'\n","    dfName[nameTemp]=-1\n","    if 'fund' in col:\n","      dfName.loc[((dfName[col]>=1) & (dfName[col] < 40)),nameTemp] = 0\n","      dfName.loc[((dfName[col]>=40) & (dfName[col] < .0)),nameTemp] = 1\n","      dfName.loc[((dfName[col]>=70) & (dfName[col] <= 100)),nameTemp] = 2\n","    else:\n","      dfName.loc[((dfName[col]>=.01) & (dfName[col] < .40)),nameTemp] = 0\n","      dfName.loc[((dfName[col]>=.40) & (dfName[col] < .70)),nameTemp] = 1\n","      dfName.loc[((dfName[col]>=.70) & (dfName[col] <= 1)),nameTemp] = 2\n","\n","  listTemp = funcColName(dfFundAllocations)\n","  listTemp.remove('uniqueID')\n","  dfStdTemp = pd.DataFrame(dfName[listTemp].std(axis = 1, skipna = True), columns = ['PortfolioAllocationStd_FeatureEngineering'])\n","  dfName = pd.concat((dfName,dfStdTemp), axis = 1)\n","\n","  dfName['LiquidityPercentage_FeatureEngineering'] = 100/(dfName['bond_percentage_of_porfolio'] + dfName['cash_percentage_of_portfolio'])\n","\n","  listTemp = funcColStringSearch(dfOtherSpecs, '_fund')\n","  listTemp.remove('ytd_return_fund')\n","  listTemp.remove('fund_id')\n","  dfStdTemp = pd.DataFrame(dfName[listTemp].std(axis = 1, skipna = True), columns = ['FundYearsReturnStd_FeatureEngineering'])\n","  dfName = pd.concat((dfName,dfStdTemp), axis = 1)\n","  dfStdTemp = pd.DataFrame(dfName[listTemp].mean(axis = 1, skipna = True), columns = ['FundYearsReturnMean_FeatureEngineering'])\n","  dfName = pd.concat((dfName,dfStdTemp), axis = 1)\n","\n","  listTemp = funcColStringSearch(dfOtherSpecs, '_category')\n","  listTemp.remove('ytd_return_category')\n","  listTemp.remove('fund_id')\n","  dfStdTemp = pd.DataFrame(dfName[listTemp].std(axis = 1, skipna = True), columns = ['CategoryYearsReturnStd_FeatureEngineering'])\n","  dfName = pd.concat((dfName,dfStdTemp), axis = 1)\n","  dfStdTemp = pd.DataFrame(dfName[listTemp].mean(axis = 1, skipna = True), columns = ['CategoryYearsReturnMean_FeatureEngineering'])\n","  dfName = pd.concat((dfName,dfStdTemp), axis = 1)\n","\n","  return dfName\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZPSAkGwSRx8x"},"source":["#### Engineering_2"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KLQwdrnFQ1jB","colab":{}},"source":["def funcFeatureEngineering_2(dfName, trainFuncSet):\n","  '''\n","  Feature Engineering function 2\n","  '''\n","  df = dfName\n","  df['postiveYears_FeatureEngineer_2'] = -999\n","  df.loc[((df['years_up'] != -999) & (df['years_down'] != -999), 'postiveYears_FeatureEngineer_2' )] = df['years_up'] - df['years_down']\n","\n","  df['tempMaxSelectionNA'] = 0.000001 #Used when all values in a col is zero for selecting the max\n","  colList = dfFundAllocations.columns.tolist()\n","  colList.remove('uniqueID')\n","  colList.append('tempMaxSelectionNA')\n","  df['maxAllocation_FeatureEngineer'] = df[colList].idxmax(axis=1)\n","\n","  colList = ['stock_percentage_of_portfolio', 'bond_percentage_of_porfolio', 'cash_percentage_of_portfolio', 'tempMaxSelectionNA']\n","  df['maxPortfolioAllocation_FeatureEngineer'] = df[colList].idxmax(axis=1)\n","\n","  colList = ['portfolio_convertable', 'portfolio_others','portfolio_preferred','tempMaxSelectionNA']\n","  df['maxPortfolioStockSelection_FeatureEngineer'] = df[colList].idxmax(axis=1)\n","\n","  df['2010_FeatureEngineering'] = df['2010_return_fund'] - df['2010_return_category'] \n","  df['2011_FeatureEngineering'] = df['2011_return_fund'] - df['2011_return_category'] \n","  df['2012_FeatureEngineering'] = df['2012_return_fund'] - df['2012_return_category'] \n","  df['2013_FeatureEngineering'] = df['2013_return_fund'] - df['2013_return_category'] \n","  df['2014_FeatureEngineering'] = df['2014_return_fund'] - df['2014_return_category'] \n","  df['2015_FeatureEngineering'] = df['2015_return_fund'] - df['2015_return_category'] \n","  df['2016_FeatureEngineering'] = df['2016_return_fund'] - df['2016_return_category'] \n","  df['2017_FeatureEngineering'] = df['2017_return_fund'] - df['2017_return_category'] \n","  df['2018_FeatureEngineering'] = df['2018_return_fund'] - df['2018_return_category'] \n","\n","  df['2010_FundComp_FeatureEngineering'] = df['2010_return_fund'] - trainFuncSet['2010_return_fund'].mean(skipna = True)\n","  df['2011_FundComp_FeatureEngineering'] = df['2011_return_fund'] - trainFuncSet['2011_return_fund'].mean(skipna = True) \n","  df['2012_FundComp_FeatureEngineering'] = df['2012_return_fund'] - trainFuncSet['2012_return_fund'].mean(skipna = True)\n","  df['2013_FundComp_FeatureEngineering'] = df['2013_return_fund'] - trainFuncSet['2013_return_fund'].mean(skipna = True)\n","  df['2014_FundComp_FeatureEngineering'] = df['2014_return_fund'] - trainFuncSet['2014_return_fund'].mean(skipna = True)\n","  df['2015_FundComp_FeatureEngineering'] = df['2015_return_fund'] - trainFuncSet['2015_return_fund'].mean(skipna = True)\n","  df['2016_FundComp_FeatureEngineering'] = df['2016_return_fund'] - trainFuncSet['2016_return_fund'].mean(skipna = True)\n","  df['2017_FundComp_FeatureEngineering'] = df['2017_return_fund'] - trainFuncSet['2017_return_fund'].mean(skipna = True)\n","  df['2018_FundComp_FeatureEngineering'] = df['2018_return_fund'] - trainFuncSet['2018_return_fund'].mean(skipna = True)\n","\n","  df['Return1_FeatureEngineering'] = df['fund_return_1months'] - df['category_return_1month'] \n","  df['Return2_FeatureEngineering'] = df['fund_return_3months'] - df['category_return_3month'] \n","  df['Return3_FeatureEngineering'] = df['fund_return_1years'] - df['category_return_1year'] \n","\n","  df['Return1_FundComp_FeatureEngineering'] = df['fund_return_1months'] - trainFuncSet['fund_return_1months'].mean(skipna = True)\n","  df['Return2_FundComp_FeatureEngineering'] = df['fund_return_3months'] - trainFuncSet['fund_return_3months'].mean(skipna = True) \n","  df['Return3_FundComp_FeatureEngineering'] = df['fund_return_1years'] - trainFuncSet['fund_return_1years'].mean(skipna = True) \n","\n","  df['Return4_FeatureEngineering'] = df['ytd_return_fund'] - df['ytd_return_category'] \n","  df['AnnualExpenseComparison_FeatureEngineering'] = df['fund_ratio_net_annual_expense']  - df['category_ratio_net_annual_expense']\n","\n","  df['Return4_FundComp_FeatureEngineering'] = df['ytd_return_fund'] - trainFuncSet['ytd_return_fund'].mean(skipna = True) \n","  df['AnnualExpenseComparison_FundComp_FeatureEngineering'] = trainFuncSet['fund_ratio_net_annual_expense']  - df['fund_ratio_net_annual_expense'].mean(skipna = True) \n","\n","  listcol = ['2010_FeatureEngineering', \n","              '2011_FeatureEngineering', \n","              '2012_FeatureEngineering', \n","              '2013_FeatureEngineering', \n","              '2014_FeatureEngineering', \n","              '2015_FeatureEngineering', \n","              '2016_FeatureEngineering', \n","              '2017_FeatureEngineering', \n","              '2018_FeatureEngineering', \n","              'Return1_FeatureEngineering', \n","              'Return2_FeatureEngineering', \n","              'Return3_FeatureEngineering', \n","              'Return4_FeatureEngineering', \n","              'AnnualExpenseComparison_FeatureEngineering']\n","\n","  # df[listcol]= df[listcol].fillna(value = -999)\n","\n","  df = funcYearFeatureEngineering(dfReturn10year, df,trainFuncSet1 = trainFuncSet, year = '10_')\n","  df = funcYearFeatureEngineering(dfReturn5year, df, trainFuncSet1 = trainFuncSet,year = '5_')\n","  df = funcYearFeatureEngineering(dfReturn3year, df, trainFuncSet1 = trainFuncSet,year = '3_')\n","  df['10_return_YearReturn_FeatureEngineering'] = df['10_years_return_fund'] - df ['10_years_return_category']\n","  df['5_return_YearReturn_FeatureEngineering'] = df['5_years_return_fund'] - df ['5_years_return_category']\n","  df['3_return_YearReturn_FeatureEngineering'] = df['fund_return_3years'] - df ['3_years_return_category']\n","\n","  listTemp = funcColStringSearch(dfMainFeatures,'rating')\n","  listTemp.remove('fund_id')\n","  listTemp.remove('us_govt_bond_rating')\n","  listTemp.append('tempMaxSelectionNA')\n","  df['maxRating_FeatureEngineer'] = df[listTemp].idxmax(axis=1)\n","\n","  listcol = ['2010_return_fund',\n","            '2011_return_fund',\n","            '2012_return_fund',\n","            '2013_return_fund',\n","            '2014_return_fund',\n","            '2015_return_fund',\n","            '2016_return_fund',\n","            '2017_return_fund',\n","            '2018_return_fund',\n","            'fund_return_1months', 'fund_return_3months', 'fund_return_1years', 'ytd_return_fund', 'fund_ratio_net_annual_expense'\n","            ]\n","\n","  listCategory = np.unique(np.array(trainFuncSet['category'])).tolist()\n","  for colName in listcol:\n","    tempName = colName + '_FundCategoryReturn_FeatureEngineering'\n","    df[tempName] = 0\n","    for category in listCategory:\n","      categoryMean = trainFuncSet.loc[trainFuncSet['category'] == category,[colName]].mean(skipna = True)\n","      df.loc[df['category']==category, tempName] =   df.loc[df['category']==category, colName] - categoryMean[0]\n","    df[tempName] = df[tempName].fillna(value= 0.00000001)\n","\n","  df = df.drop(columns = ['tempMaxSelectionNA'])\n","\n","  return df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TONzBbS7JYp0"},"source":["## Impute Function"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y3l4EPh7d8Db"},"source":["### Section 1 \n","\n","1.   funcImputeGroupBy()\n","2.   funcTreatingXyears()\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jndgEEdhs7Xm"},"source":["#### funcTreatingXyears\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1lMaXWW1eDAE","colab":{}},"source":["def funcTreatingXyears(dfName,colSearchDF ,treatYears, trainSet = True, dictImpute = {}, dictImputeValue = '',trainNAValue = -1):\n","  '''\n","  Imputes 0 to all the funds whose age is less than treatYears years and Imputes mean for the remaining NaN values\n","  '''\n","  if trainSet == True:\n","    listTempColName = funcColStringSearch(colSearchDF, str(treatYears))\n","    # listColToRemove = funcColStringSearch(colSearchDF, 'category')\n","    # listTempColName = funcListItemRemovals(listTempColName,listColToRemove) \n","    listTempColName.remove('fund_id') \n","    dfName.loc[(baseYear - dfName['inception_date'].dt.year >= treatYears),listTempColName] = dfName.loc[(baseYear - dfName['inception_date'].dt.year >= treatYears),\n","                                                                                                                      listTempColName].fillna(value = -999)\n","    dfName.loc[(baseYear - dfName['inception_date'].dt.year < treatYears),listTempColName] = 0    \n","  else:\n","    listTempColName = funcColStringSearch(colSearchDF, str(treatYears))\n","    listTempColName.remove('fund_id') \n","    dfName.loc[(baseYear - dfName['inception_date'].dt.year >= treatYears),listTempColName] = dfName.loc[(baseYear - dfName['inception_date'].dt.year >= treatYears),\n","                                                                                                                      listTempColName].fillna(value = -999) \n","    dfName.loc[(baseYear - dfName['inception_date'].dt.year < treatYears),listTempColName] = 0   \n","  return dfName\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JYfcChE5eok6"},"source":["### Section 2"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0U96_SoLdcBt"},"source":["#### funcImpute_()"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2-k1ME2jaArz","colab":{}},"source":["'''\n","Treats Missing values of differnet dataframes\n","'''\n","\n","# # # dfBondRatings # # #\n","def funcImpute_dfBondRatings(dfName,dictImpute = {},trainSet = True,trainNAValue = -1):\n","  if trainSet == True:\n","    listTempColName = funcColStringSearch(dfName,'_rating', lastWordSearch = 'last')\n","    listTempColName.remove('fund_id')\n","    dfName[listTempColName] = dfName[listTempColName].fillna(value= trainNAValue)\n","  else:\n","    listTempColName = funcColStringSearch(dfName,'_rating', lastWordSearch = 'last')\n","    listTempColName.remove('fund_id')\n","    dfName[listTempColName] = dfName[listTempColName].fillna(value= dictImpute['dfBondRatings'])\n","  return dfName\n","\n","# # # dfFundAllocations # # #\n","def funcImpute_dfFundAllocations(dfName,dictImpute = {},trainSet = True,trainNAValue = -1):\n","  if trainSet == True:\n","    listTempColName = funcColName(dfFundAllocations)\n","    listTempColName.remove('uniqueID')\n","    dfName[listTempColName] = dfName[listTempColName].fillna(value= trainNAValue)\n","  else:\n","    listTempColName = funcColName(dfFundAllocations)\n","    listTempColName.remove('uniqueID')\n","    dfName[listTempColName] = dfName[listTempColName].fillna(value= dictImpute['dfFundAllocations'])\n","  return dfName\n","\n","# # # dfFundRatios # # #\n","def funcImpute_dfFundRatios(dfName,dictImpute = {},trainSet = True,trainNAValue = -1):\n","  if trainSet == True:\n","    listTempColName = funcColName(dfFundRatios)\n","    listRemoveItems = ['fund_id','uniqueID', 'fund_ratio_net_annual_expense']\n","    listTempColName = funcListItemRemovals(listTempColName,listRemoveItems)\n","    dfName[listTempColName] = dfName[listTempColName].fillna(value= trainNAValue) \n","  else:\n","    listTempColName = funcColName(dfFundRatios)\n","    listRemoveItems = ['fund_id','uniqueID', 'fund_ratio_net_annual_expense']\n","    listTempColName = funcListItemRemovals(listTempColName,listRemoveItems)\n","    dfName[listTempColName] = dfName[listTempColName].fillna(value= dictImpute['dfFundRatios'])\n","  return dfName\n","\n","# # # dfFundSpecs # # # \n","def funcImpute_dfFundSpecs(dfName,dictImpute = {},trainSet = True,trainNAValue = -1):\n","  if trainSet == True:\n","    # Categorical Columns Treating dfFundSpecs\n","    listTempColName = funcColName(dfFundSpecs)\n","    listRemoveItems = ['uniqueID', 'greatstone_rating']\n","    listTempColName = funcListItemRemovals(listTempColName,listRemoveItems)\n","    for colName in listTempColName:\n","      if colName in dfName.columns and dfName[colName].dtype == 'object':\n","        dfName.loc[dfName[colName].isnull(),colName] = 'UnKnown'    \n","    #Imputing Values\n","    listTempColName = ['return_ytd', 'yield', 'total_assets']\n","    dfName[listTempColName] = dfName[listTempColName].fillna(value= trainNAValue)\n","  else:\n","    # Categorical Columns Treating dfFundSpecs\n","    listTempColName = funcColName(dfFundSpecs)\n","    listRemoveItems = ['uniqueID', 'greatstone_rating']\n","    listTempColName = funcListItemRemovals(listTempColName,listRemoveItems)\n","    for colName in listTempColName:\n","      if colName in dfName.columns and dfName[colName].dtype == 'object':\n","        dfName.loc[dfName[colName].isnull(),colName] = 'UnKnown'   \n","    #Imputing Values   \n","    listTempColName = ['return_ytd', 'yield', 'total_assets']\n","    dfName[listTempColName] = dfName[listTempColName].fillna(value= dictImpute['dfFundSpecs'])\n","  return dfName\n","\n","# # # dfOtherSpecs # # # \n","def funcImpute_dfOtherSpecs(dfName,dictImpute = {},trainSet = True,trainNAValue = -1):\n","  listYear = [2010,2011,2012,2013,2014,2015,2016,2017,2018]\n","  if trainSet == True:\n","    for yearItem in listYear:\n","      listTempColName = funcColStringSearch(dfOtherSpecs,str(yearItem), 'first')\n","      listTempColName.remove('fund_id')\n","      dfName.loc[(dfName['inception_date'].dt.year > yearItem),listTempColName] = dfName.loc[(dfName['inception_date'].dt.year > yearItem),\n","                                                                                              listTempColName].fillna(value = 0)                                                                                                                                                              \n","    for yearItem in listYear:\n","      listTempColName = funcColStringSearch(dfOtherSpecs,str(yearItem), 'first')\n","      listTempColName.remove('fund_id')\n","      dfName[listTempColName] = dfName[listTempColName].fillna(value= trainNAValue)\n","\n","    dfName.loc[dfName['inception_date'].dt.year == 2019, 'years_up'] = 0\n","    dfName.loc[dfName['inception_date'].dt.year == 2019, 'years_down'] = 0  \n","\n","    listTempColName = ['category_return_3month', 'fund_return_1years', 'category_return_1year', 'fund_return_3years', 'ytd_return_fund', 'ytd_return_category',\n","                        'fund_return_3months', 'category_return_1month', 'fund_return_1months', 'cash_percentage_of_portfolio', 'bond_percentage_of_porfolio',\n","                        'portfolio_preferred', 'stock_percentage_of_portfolio','portfolio_others', 'portfolio_convertable',  'years_down', 'years_up']\n","    dfName[listTempColName] = dfName[listTempColName].fillna(value= trainNAValue)\n","  else:\n","    for yearItem in listYear:\n","      listTempColName = funcColStringSearch(dfOtherSpecs,str(yearItem), 'first')\n","      listTempColName.remove('fund_id')\n","      dfName.loc[(dfName['inception_date'].dt.year > yearItem),listTempColName] = dfName.loc[(dfName['inception_date'].dt.year > yearItem),\n","                                                                                                                                    listTempColName].fillna(value = 0)                                                                                                                       \n","    for yearItem in listYear:\n","      listTempColName = funcColStringSearch(dfOtherSpecs,str(yearItem), 'first')\n","      listTempColName.remove('fund_id')\n","      dfName[listTempColName] = dfName[listTempColName].fillna(value= trainNAValue)\n","\n","    dfName.loc[dfName['inception_date'].dt.year == 2019, 'years_up'] = 0\n","    dfName.loc[dfName['inception_date'].dt.year == 2019, 'years_down'] = 0 \n","\n","    listTempColName = ['category_return_3month', 'fund_return_1years', 'category_return_1year', 'fund_return_3years', 'ytd_return_fund', 'ytd_return_category',\n","                        'fund_return_3months', 'category_return_1month', 'fund_return_1months', 'cash_percentage_of_portfolio', 'bond_percentage_of_porfolio',\n","                        'portfolio_preferred', 'stock_percentage_of_portfolio','portfolio_others', 'portfolio_convertable',  'years_down', 'years_up']\n","    dfName[listTempColName] = dfName[listTempColName].fillna(value= dictImpute['dfOtherSpecs'])\n","  return dfName\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mVej7Zfcjbcy"},"source":["#### funcImputeCentraltendancy()"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q48DxwIpjbxb","colab":{}},"source":["def funcImputeCentraltendancy(df, trainSet = True, dictMain = {}):\n","  '''\n","  This function imputes missing values which were replaced by -999 using funcimpute_() functions with central tendancy of the column. \n","  '''\n","  dfName = df\n","  if trainSet == True:\n","    dictCentralTendancy = {}\n","    for col in dfName.columns:\n","      if col != 'fund_id':\n","        naValue = dfName.loc[dfName[col]!=-999,col].mean()\n","        dfName.loc[dfName[col]==-999,[col]] = naValue\n","        dictCentralTendancy[col] = naValue\n","        #dictCentralTendancy.update({col: naValue})\n","    return dfName,dictCentralTendancy\n","  else:\n","    for col in dfName.columns:\n","      # Test Set\n","      if col != 'fund_id':\n","        naValue = dictMain[col]\n","        dfName.loc[dfName[col]==-999,[col]] = naValue\n","    return dfName"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iAuQbzVvsnvq"},"source":["#### funcImputeMain()"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AjfNZgxufDXc","colab":{}},"source":["def funcImputeMain(dfMainPreprocessed, boolTrainSetMain = True, dictImputeMain = {}):\n","  '''\n","  This funtion imputes values using the funcimpute_() function.\n","  '''\n","  dictMain = {}\n","  # # # Treating dfBondRatings # # #\n","  print('Treating dfBondRatings')\n","  naValue = -999\n","  dfMainPreprocessed = funcImpute_dfBondRatings(dfMainPreprocessed, dictImpute = dictImputeMain,trainSet = boolTrainSetMain,trainNAValue = naValue)\n","  dictMain.update({'dfBondRatings' : naValue})\n","\n","  # # # Treating dfFundAllocations # # #\n","  print('Treating dfFundAllocations')\n","  naValue = -999                            # 9.09\n","  dfMainPreprocessed = funcImpute_dfFundAllocations(dfMainPreprocessed, dictImpute = dictImputeMain, trainSet = boolTrainSetMain,trainNAValue = naValue)\n","  dictMain.update({'dfFundAllocations' : naValue})\n","\n","  # # # Treating dfFundRatios # # #\n","  print('Treating dfFundRatios')\n","  naValue = -999\n","  dfMainPreprocessed = funcImpute_dfFundRatios(dfMainPreprocessed, dictImpute = dictImputeMain, trainSet = boolTrainSetMain,trainNAValue = naValue)\n","  dictMain.update({'dfFundRatios' : naValue})\n","\n","  # # # Treating dfFundSpecs # # #\n","  print('Treating dfFundRatios')\n","  naValue = -999\n","  dfMainPreprocessed = funcImpute_dfFundSpecs(dfMainPreprocessed, dictImpute = dictImputeMain, trainSet = boolTrainSetMain,trainNAValue = naValue)\n","  dictMain.update({'dfFundSpecs' : naValue})\n","\n","  # # # Treating dfOtherSpecs # # #\n","  print('Treating dfOtherSpecs')\n","  naValue = -999\n","  dfMainPreprocessed = funcImpute_dfOtherSpecs(dfMainPreprocessed, dictImpute = dictImputeMain, trainSet = boolTrainSetMain,trainNAValue = naValue)\n","  dictMain.update({'dfOtherSpecs' : naValue})\n","\n","  # # # Treating dfReturn10year # # #\n","  print('Treating dfReturn10year')\n","  dfMainPreprocessed = funcTreatingXyears(dfMainPreprocessed,dfReturn10year ,10, trainSet = boolTrainSetMain, \n","                                          dictImpute = dictImputeMain, dictImputeValue = 'dfReturn10year',trainNAValue = -999)\n","  dictMain.update({'dfReturn10year' : naValue})\n","\n","  # # # dfReturn3year # # #\n","  print('Treating dfReturn3year')\n","  dfMainPreprocessed = funcTreatingXyears(dfMainPreprocessed,dfReturn3year ,3, trainSet = boolTrainSetMain, \n","                                          dictImpute = dictImputeMain, dictImputeValue = 'dfReturn3year',trainNAValue = -999)\n","  dictMain.update({'dfReturn3year' : naValue})\n","\n","  # # # dfReturn5year # # #\n","  print('Treating dfReturn5year')\n","  dfMainPreprocessed = funcTreatingXyears(dfMainPreprocessed,dfReturn5year ,5, trainSet = boolTrainSetMain, \n","                                          dictImpute = dictImputeMain, dictImputeValue = 'dfReturn5year',trainNAValue = -999)\n","  dictMain.update({'dfReturn5year' : naValue})\n","  if boolTrainSetMain == True:\n","    return dfMainPreprocessed, dictMain\n","  else:\n","    return dfMainPreprocessed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MjqrhsHZyehz"},"source":["## Outlier Removal Function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ulSl44DEyky3","colab":{}},"source":["def funcCapOutlierData(dfName, lowerQuantile =.25, upperQuantile = .75, trainSet = True, dictOutlier = {}):\n","  '''\n","  This function caps the outliers value with 25 percentile value and 75 percentile value\n","  '''\n","  if trainSet == True:\n","    dictOutlierTuple = {}\n","    if lowerQuantile > 1:\n","      lowerQuantile = lowerQuantile/100\n","    if upperQuantile > 1:\n","      upperQuantile = upperQuantile/100\n","    for col in dfName.columns:\n","        if (((dfName[col].dtype)=='float64') | ((dfName[col].dtype)=='int64')):\n","            percentiles = dfName[col].quantile([lowerQuantile,upperQuantile]).values\n","            lowerQuantileValue = percentiles[0]\n","            upperQuantileValue = percentiles[1]\n","            dfName.loc[dfName[col] <= lowerQuantileValue,col] = lowerQuantileValue\n","            dfName.loc[dfName[col] >= upperQuantileValue, col] = upperQuantileValue\n","            dictOutlierTuple[col] = (lowerQuantileValue, upperQuantileValue)\n","        # else:\n","        #     dfName[col]=dfName[col]\n","    return dfName, dictOutlierTuple\n","  else:\n","    for col in dfName.columns:\n","        if (((dfName[col].dtype)=='float64') | ((dfName[col].dtype)=='int64')):\n","          if col in dictOutlier.keys():\n","            lowerQuantileValue = dictOutlier[col][0]\n","            upperQuantileValue = dictOutlier[col][1]\n","            dfName.loc[dfName[col] <= lowerQuantileValue,col] = lowerQuantileValue\n","            dfName.loc[dfName[col] >= upperQuantileValue, col] = upperQuantileValue\n","          else:\n","            print(col,' not present in train dataset')\n","        # else:\n","        #     dfName[col]=dfName[col]\n","    return dfName"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oswAGb2nfKXF"},"source":["## funcCustomPipeLinePreprocessing\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hRvDacebFNC0","colab":{}},"source":["def funcCustomPipeLinePreprocessing(dfMain,trainFuncSetMain, trainSet = True, NaNThreshold = 50, removeRows = False,listColToRemoveCustom = [], dictImputeCustom = {}, dictOutlierCustom = {}, \n","                                    dictOneHotCategoriesCustom={}, dictCentralTendancy = {}):\n","  '''\n","  * This function is used to ensure same preprocessing methods are applied in order for both the train data set and test dataset.\n","  * Returns the scalar values the were used for the Main dataframe to be used on the test dataset.\n","  '''\n","  dictOneHotCategories = {}\n","  if trainSet == True:\n","    # # # Columns To Remove # # #\n","    print('Removing columns')\n","    listColSameValue = dfMain.columns[dfMain.nunique() <= 1].tolist() # List of columns that have same value in them\n","    listColNaN = funcIsNull(dfMain,True,NaNThreshold) # List of columns that have NaN more than or equal to 50%\n","    listColToRemove = listColSameValue + listColNaN\n","    listColToRemove.append('fund_name')\n","    # listColToRemove.append('parent_company')\n","    # listColToRemove.append('category')\n","    dfMainPreprocessed = dfMain.drop(columns = listColToRemove)\n","\n","    # # # Rows To Remove # # # \n","    # ******** Not for the actual test dataset ********#\n","    listRowsToRemove = []\n","    if True == removeRows:\n","      listRowsToRemove = funcIsNull(dfMain,False,50)\n","      dfMainPreprocessed = dfMainPreprocessed[~dfMainPreprocessed['fund_id'].isin(listRowsToRemove)] # Removes the rows that have more than 50% NaN in them.\n","\n","    print('One Hot Encoding Part 1')\n","    dfMainPreprocessed,listCategoriesTop = funcTopXOneHotEncode(dfMainPreprocessed, 'parent_company', top = 10, sortAscending = False,dropOriginal = False) # Top X OnehotEncoding 'parent_company' feature\n","    dfMainPreprocessed,listCategoriesBottom = funcTopXOneHotEncode(dfMainPreprocessed, 'parent_company', top = 10, sortAscending = True,dropOriginal = False) # Top X bottom\n","    dictOneHotCategories.update({'parent_company':(listCategoriesTop,listCategoriesBottom)})\n","    \n","    dfMainPreprocessed,listCategoriesTop = funcTopXOneHotEncode(dfMainPreprocessed, 'category', top = 10,sortAscending = False,dropOriginal = False) # Top X OnehotEncoding 'category' feature\n","    dfMainPreprocessed,listCategoriesBottom = funcTopXOneHotEncode(dfMainPreprocessed, 'category', top = 10, sortAscending = True,dropOriginal = False) # Top X bottom\n","    dictOneHotCategories.update({'category':(listCategoriesTop,listCategoriesBottom)})\n","\n","    # # Treating Outliers # # #\n","    print('Treating Outlier')\n","    dfMainPreprocessed, dictOutlierCapTuple = funcCapOutlierData(dfMainPreprocessed,trainSet = True)\n","\n","    # # # Feature Engineering # # #\n","    print('Feature Engineering 1')\n","    dfMainPreprocessed = funcFeatureEngineering(dfMainPreprocessed)\n","    \n","    # # # Imputation # # #\n","    print('Imputation')\n","    dfMainPreprocessed, dictMain = funcImputeMain(dfMainPreprocessed)\n","\n","    # # # Feature Engineering # # # -> The below steps are after imputaion because we have imputed new categories\n","    print('Feature Engineering 2')\n","    dfMainPreprocessed = funcFeatureEngineering_2(dfMainPreprocessed,trainFuncSet = trainFuncSetMain)\n","    print('Done')\n","\n","    dfMainPreprocessed = dfMainPreprocessed.drop(columns = ['parent_company', 'category'])\n","\n","    print('One Hot Encoding Part 2')\n","    dfMainPreprocessed, listCategories= funcTopXOneHotEncode(dfMainPreprocessed, 'investment_class', dropOriginal = True) # Top X OnehotEncoding 'category' feature\n","    dictOneHotCategories.update({'investment_class':listCategories})\n","\n","    labelencoder = LabelEncoder()\n","    dfMainPreprocessed['maxAllocation_FeatureEngineer'] = labelencoder.fit_transform(dfMainPreprocessed['maxAllocation_FeatureEngineer'])\n","    dictOneHotCategories.update({'maxAllocation_FeatureEngineer':labelencoder})\n","    \n","    labelencoder = LabelEncoder()\n","    dfMainPreprocessed['maxPortfolioAllocation_FeatureEngineer'] = labelencoder.fit_transform(dfMainPreprocessed['maxPortfolioAllocation_FeatureEngineer'])\n","    dictOneHotCategories.update({'maxPortfolioAllocation_FeatureEngineer':labelencoder})\n","\n","    labelencoder = LabelEncoder()\n","    dfMainPreprocessed['maxRating_FeatureEngineer'] = labelencoder.fit_transform(dfMainPreprocessed['maxRating_FeatureEngineer'])\n","    dictOneHotCategories.update({'maxRating_FeatureEngineer':labelencoder})\n","\n","    labelencoder = LabelEncoder()\n","    dfMainPreprocessed['maxPortfolioStockSelection_FeatureEngineer'] = labelencoder.fit_transform(dfMainPreprocessed['maxPortfolioStockSelection_FeatureEngineer'])\n","    dictOneHotCategories.update({'maxPortfolioStockSelection_FeatureEngineer':labelencoder})\n","\n","\n","    dfMainPreprocessed, listCategories = funcTopXOneHotEncode(dfMainPreprocessed, 'fund_size', dropOriginal = True) # Top X OnehotEncoding 'category' feature\n","    dictOneHotCategories.update({'fund_size':listCategories})\n","    # labelencoder = LabelEncoder()\n","    # dfMainPreprocessed['fund_size'] = labelencoder.fit_transform(dfMainPreprocessed['fund_size'])\n","    # dictOneHotCategories.update({'fund_size':labelencoder})\n","\n","    listNaNCol = [col for col in pd.DataFrame(dfMainPreprocessed).columns if pd.DataFrame(dfMainPreprocessed)[col].isnull().any()]\n","    dfMainPreprocessed[listNaNCol] = dfMainPreprocessed[listNaNCol].fillna(value= -999)\n","\n","    # # Central Tendancy Imputation # # #\n","    print('Imputing Central Tendancy')\n","    dfMainPreprocessed, dictMainCentralTendancy = funcImputeCentraltendancy(dfMainPreprocessed, trainSet = trainSet, dictMain = {})\n","\n","    return dfMainPreprocessed, listColToRemove, dictMain, dictOutlierCapTuple, dictOneHotCategories, dictMainCentralTendancy\n","\n","  else:\n","\n","    # # # Removing Columns # # #\n","    dfMainPreprocessed = dfMain.drop(columns = listColToRemoveCustom)\n","\n","    print('One Hot Encoding Part 1')\n","    dfMainPreprocessed = funcTopXOneHotEncode(dfMainPreprocessed, 'parent_company', dropOriginal = False,trainSet=False,listFeatureCategories=dictOneHotCategoriesCustom['parent_company'][0]) # Top X OnehotEncoding 'parent_company' feature\n","    dfMainPreprocessed = funcTopXOneHotEncode(dfMainPreprocessed, 'parent_company', dropOriginal = False,trainSet=False,listFeatureCategories=dictOneHotCategoriesCustom['parent_company'][1]) \n","  \n","    dfMainPreprocessed = funcTopXOneHotEncode(dfMainPreprocessed, 'category',  dropOriginal = False,trainSet=False,listFeatureCategories=dictOneHotCategoriesCustom['category'][0]) # Top X OnehotEncoding 'category' feature\n","    dfMainPreprocessed = funcTopXOneHotEncode(dfMainPreprocessed, 'category',  dropOriginal = False,trainSet=False,listFeatureCategories=dictOneHotCategoriesCustom['category'][1])\n","    \n","    # # # Treating Outliers # # #\n","    print('Treating Outlier')\n","    dfMainPreprocessed = funcCapOutlierData(dfMainPreprocessed, trainSet = trainSet, dictOutlier = dictOutlierCustom)\n","\n","    # # # Feature Engineering # # #\n","    print('Feature Engineering 1')\n","    dfMainPreprocessed = funcFeatureEngineering(dfMainPreprocessed)\n","\n","    # # # Imputation # # #\n","    print('Imputation')\n","    dfMainPreprocessed = funcImputeMain(dfMainPreprocessed, boolTrainSetMain = trainSet, dictImputeMain = dictImputeCustom)\n","\n","    print('Feature Engineering 2')\n","    dfMainPreprocessed = funcFeatureEngineering_2(dfMainPreprocessed,trainFuncSet = trainFuncSetMain)\n","\n","    dfMainPreprocessed = dfMainPreprocessed.drop(columns = ['parent_company', 'category'])\n","\n","    # The below steps are after imputaion because we have imputed new categories\n","    print('One Hot Encoding Part 2')\n","    dfMainPreprocessed = funcTopXOneHotEncode(dfMainPreprocessed, 'investment_class', dropOriginal = True,trainSet=False,listFeatureCategories=dictOneHotCategoriesCustom['investment_class']) # Top X OnehotEncoding 'category' feature\n","\n","    dfMainPreprocessed = funcTopXOneHotEncode(dfMainPreprocessed, 'fund_size', dropOriginal = True,trainSet=False,listFeatureCategories=dictOneHotCategoriesCustom['fund_size']) # Top X OnehotEncoding 'category' feature\n","\n","    labelEncoderTest = dictOneHotCategoriesCustom['maxAllocation_FeatureEngineer']\n","    dfMainPreprocessed['maxAllocation_FeatureEngineer'] = dfMainPreprocessed['maxAllocation_FeatureEngineer'].map(lambda s: '<unknown>' if s not in labelEncoderTest.classes_ else s)\n","    labelEncoderTest.classes_ = np.append(labelEncoderTest.classes_, '<unknown>')\n","    dfMainPreprocessed['maxAllocation_FeatureEngineer'] = labelEncoderTest.transform(dfMainPreprocessed['maxAllocation_FeatureEngineer'])\n","    \n","    labelEncoderTest = dictOneHotCategoriesCustom['maxPortfolioAllocation_FeatureEngineer']\n","    dfMainPreprocessed['maxPortfolioAllocation_FeatureEngineer'] = dfMainPreprocessed['maxPortfolioAllocation_FeatureEngineer'].map(lambda s: '<unknown>' if s not in labelEncoderTest.classes_ else s)\n","    labelEncoderTest.classes_ = np.append(labelEncoderTest.classes_, '<unknown>')\n","    dfMainPreprocessed['maxPortfolioAllocation_FeatureEngineer'] = labelEncoderTest.transform(dfMainPreprocessed['maxPortfolioAllocation_FeatureEngineer'])\n","    \n","    labelEncoderTest = dictOneHotCategoriesCustom['maxRating_FeatureEngineer']\n","    dfMainPreprocessed['maxRating_FeatureEngineer'] = dfMainPreprocessed['maxRating_FeatureEngineer'].map(lambda s: '<unknown>' if s not in labelEncoderTest.classes_ else s)\n","    labelEncoderTest.classes_ = np.append(labelEncoderTest.classes_, '<unknown>')\n","    dfMainPreprocessed['maxRating_FeatureEngineer'] = labelEncoderTest.transform(dfMainPreprocessed['maxRating_FeatureEngineer'])\n","\n","    labelEncoderTest = dictOneHotCategoriesCustom['maxPortfolioStockSelection_FeatureEngineer']\n","    dfMainPreprocessed['maxPortfolioStockSelection_FeatureEngineer'] = dfMainPreprocessed['maxPortfolioStockSelection_FeatureEngineer'].map(lambda s: '<unknown>' if s not in labelEncoderTest.classes_ else s)\n","    labelEncoderTest.classes_ = np.append(labelEncoderTest.classes_, '<unknown>')\n","    dfMainPreprocessed['maxPortfolioStockSelection_FeatureEngineer'] = labelEncoderTest.transform(dfMainPreprocessed['maxPortfolioStockSelection_FeatureEngineer'])\n","\n","    listNaNCol = [col for col in pd.DataFrame(dfMainPreprocessed).columns if pd.DataFrame(dfMainPreprocessed)[col].isnull().any()]\n","    dfMainPreprocessed[listNaNCol] = dfMainPreprocessed[listNaNCol].fillna(value= -999)\n","    \n","    # # Central Tendancy Imputation # # #\n","    print('Imputing Central Tendancy')\n","    dfMainPreprocessed = funcImputeCentraltendancy(dfMainPreprocessed, trainSet = False, dictMain = dictCentralTendancy)\n","\n","    return dfMainPreprocessed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xNjPbcGrHwxa"},"source":["# Visualizations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wBvaany4_B9N","colab":{}},"source":["# # # Class Imbalance # # #\n","dfMainLabels['greatstone_rating'].value_counts().plot(kind = 'bar', color = 'r', label = 'Count')\n","plt.title('Class Imbalance')\n","plt.xlabel('Classes')\n","plt.ylabel('Counts')\n","plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wXnN7m4N3Jex","colab":{}},"source":["def funcFeatureImportance(model, featureNums = -20):\n","  '''\n","  Visualizes important features\n","  '''\n","  features = cvTrain.columns\n","  importance = model.feature_importances_\n","  indices = np.argsort(importance)\n","\n","  plt.figure(figsize=(10,15)) \n","  plt.title('Feature Importances')\n","  plt.barh(range(len(indices[featureNums:])), importance[indices[featureNums:]], color='b', align='center')\n","  plt.yticks(range(len(indices[featureNums:])), [features[i] for i in indices[featureNums:]])\n","  plt.xlabel('Relative Importance')\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5OE5mld2Sjdg"},"source":["# Main"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kUQIVe5VBxCh","colab":{}},"source":["if __name__ == '__main__':\n","\n","  if True == True:\n","    dfMain = pd.DataFrame()\n","    dfActualTest = pd.DataFrame()\n","    X_train = pd.DataFrame()\n","    X_test =  pd.DataFrame()\n","\n","    scaler = StandardScaler()\n","\n","    funcRenameColumn(dictDataFrames) # Renames tag and ID to uniqueID\n","    dfMain, dfActualTest= funcTestSetSplit(funcConcatDF(dictDataFrames)) # Function Inside an function\n","    dfMainFeatures = dfMain.loc[:, ~dfMain.columns.isin(['greatstone_rating'])] # Features are split from dfMain\n","    dfMainLabels = dfMain[['fund_id', 'greatstone_rating']] # Labels are split from dfMain\n","\n","    X_train, X_test, y_train, y_test = train_test_split(dfMainFeatures, dfMainLabels, test_size=0.20, random_state=333) # Splitting dfMain into Test and Train\n","    X_trainOriginal = X_train\n","    print('-----------------------Train Set---------------------\\n')\n","    X_train, listColToRemoveMain, dictImputeMain, dictOutlierCapTupleMain, dictOneHotCategoriesMain, dictMainCentralTendancy = funcCustomPipeLinePreprocessing(X_train, trainFuncSetMain = X_trainOriginal)\n","    X_train = X_train.drop(columns = ['fund_id', 'inception_date'])\n","\n","    # # # Removing co-related columns # # #\n","    corr_matrix = X_train.corr().abs()\n","    # Select upper triangle of correlation matrix\n","    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n","    # Find index of feature columns with correlation greater than 0.95\n","    to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n","    to_drop\n","    toDropNew = []\n","    for item in to_drop:\n","      if  '_FeatureEngineer' not in item and '_fund' not in item:\n","        toDropNew.append(item)\n","    X_train = X_train.drop(columns = toDropNew)\n","    y_train = y_train.drop(columns = ['fund_id'])\n","\n","    print('-----------------------Test Set---------------------\\n')\n","    X_test = funcCustomPipeLinePreprocessing(X_test,trainSet = False,listColToRemoveCustom= listColToRemoveMain, dictImputeCustom=dictImputeMain, \n","                                             dictOutlierCustom=dictOutlierCapTupleMain, dictOneHotCategoriesCustom = dictOneHotCategoriesMain, \n","                                             dictCentralTendancy = dictMainCentralTendancy,\n","                                             trainFuncSetMain = X_trainOriginal)\n","    \n","    X_test = X_test.drop(columns = ['fund_id', 'inception_date'])\n","    X_test = X_test.drop(columns = toDropNew)\n","    y_test = y_test.drop(columns = ['fund_id'])\n","\n","    print('-----------------------Actual Test Set---------------------\\n')\n","    dfTest = dfSample.drop(columns = ['greatstone_rating'])\n","    productionDF = pd.merge(dfTest, dfActualTest, on = 'fund_id', how='inner')\n","    productionDF = productionDF.drop(columns = ['greatstone_rating'])\n","    productionDFFundID = productionDF['fund_id']\n","    productionDF = funcCustomPipeLinePreprocessing(productionDF,trainSet = False,listColToRemoveCustom= listColToRemoveMain, dictImputeCustom=dictImputeMain, \n","                                             dictOutlierCustom=dictOutlierCapTupleMain,\n","                                             dictCentralTendancy = dictMainCentralTendancy, \n","                                             dictOneHotCategoriesCustom = dictOneHotCategoriesMain,\n","                                             trainFuncSetMain = X_trainOriginal)\n","    \n","    productionDF = productionDF.drop(columns = ['fund_id', 'inception_date'])\n","\n","  # # # Concatinating Train and Test Dataframes \n","  cvTrain = pd.concat([pd.DataFrame(X_train),pd.DataFrame(X_test)],ignore_index=True)\n","  cvTest = pd.concat([y_train,y_test],ignore_index=True)\n","\n","  # # # Standard Scalar\n","  print('-----------------------Standard Scalar---------------------\\n')\n","  X_train = scaler.fit_transform(cvTrain)\n","  X_test = scaler.transform(X_test)\n","  cvTrain = scaler.fit_transform(cvTrain)\n","  productionDF = scaler.transform(productionDF)\n","\n","  if True == True:\n","     print('-----------------------Stacking Model---------------------\\n')\n","    strModelName = 'StackingModel'\n","    class_weights = class_weight.compute_class_weight('balanced',\n","                                                  np.unique(y_train.values),\n","                                                  y_train['greatstone_rating']) \n","    print('Cross Validation')\n","    sampleWeight = funcCreateBalancedSampleWeights(cvTest['greatstone_rating'], .34)\n","    classiRandomForest = RandomForestClassifier(n_estimators = 30, criterion = 'entropy',random_state = 143)\n","    classiKNN_1 = KNeighborsClassifier(n_neighbors = 301, weights = 'distance',algorithm = 'ball_tree')\n","    classiKNN_2 = KNeighborsClassifier(n_neighbors = 301,algorithm = 'ball_tree')\n","    classiNaiveBayes = GaussianNB()\n","    classiXGBoost_1 = XGBClassifier(\n","                    learning_rate =0.04,\n","                    n_estimators=500,\n","                    max_depth=17,\n","                    min_child_weight=1,\n","                    subsample=0.8,\n","                    colsample_bytree=0.8,\n","                    objective= 'multi:softprob',\n","                    num_class = 6,\n","                    scale_pos_weight=1,\n","                    sample_weight = sampleWeight,\n","                    seed=27)\n","    classiXGBoost_2 = XGBClassifier(\n","                    learning_rate =0.04,\n","                    n_estimators=500,\n","                    subsample=0.8,\n","                    colsample_bytree=0.8,\n","                    max_depth=17,\n","                    objective= 'multi:softprob',\n","                    num_class = 6,\n","                    sample_weight = sampleWeight,\n","                    seed=27)\n","    classiXGBoost_3 = XGBClassifier(\n","                    learning_rate =0.04,\n","                    n_estimators=700,\n","                    subsample=0.8,\n","                    colsample_bytree=0.8,\n","                    max_depth=15,\n","                    objective= 'multi:softprob',\n","                    num_class = 6,\n","                    sample_weight = sampleWeight,\n","                    seed=27)\n","    classiXGBoost_4 = XGBClassifier(\n","                    learning_rate =0.04,\n","                    n_estimators=500,\n","                    max_depth=12,\n","                    colsample_bytree=0.8,\n","                    colsample_bylevel = 0.8,\n","                    colsample_bynode = 0.8,\n","                    objective= 'multi:softprob',\n","                    num_class = 6,\n","                    sample_weight = sampleWeight,\n","                    seed=27)\n","    classiXGBoost_5 = XGBClassifier(\n","                    learning_rate =0.04,\n","                    n_estimators=700,\n","                    colsample_bytree=0.8,\n","                    colsample_bylevel = 0.8,\n","                    colsample_bynode = 0.8,\n","                    max_depth=12,\n","                    objective= 'multi:softprob',\n","                    num_class = 6,\n","                    sample_weight = sampleWeight,\n","                    seed=27)\n","    classiAda = AdaBoostClassifier(\n","              DecisionTreeClassifier(max_depth=10, max_features = 'log2', min_samples_leaf = 71, \n","                                     class_weight = {0:2.41109102,1:2.47371676,2:0.79294281,3:0.49118929,4:0.71703863,5: 2.04342273}, # Weights calculated using CreateBalancedSampleWeights\n","                                     random_state = 1234),n_estimators=500)\n","    classiLR = LogisticRegression(max_iter = 5000,dual = False, random_state=0)\n","    classiLR_1 = LogisticRegression(max_iter = 5000,solver = 'lbfgs',\n","                                    dual = False, random_state=0, \n","                                    class_weight = {0:2.41109102,1:2.47371676,2:0.79294281,3:0.49118929,4:0.71703863,5: 2.04342273},\n","                                    C = .01,)\n","    classifier = StackingClassifier(classifiers=[classiAda,classiKNN_1, classiKNN_2,classiLR_1,classiXGBoost_1,classiXGBoost_2,classiXGBoost_4],\n","                                    use_probas=True,\n","                                    average_probas=False,\n","                                    verbose=2,\n","                                    use_features_in_secondary = False,\n","                                    meta_classifier=classiLR)\n","\n","    print('---------Cross-Val-Score---------')\n","    scores = cross_val_score(classifier, pd.DataFrame(cvTrain), cvTest, cv = 5 , scoring = 'f1_macro', verbose=1)\n","    print('Mean {} , Std Deviation {}'.format(scores.mean(), scores.std()))\n","\n","  if True == True:\n","    print('Fitting the model')\n","    classifier.fit(cvTrain, cvTest)\n","    print('Predecting the model')\n","    productionPrediction = classifier.predict(pd.DataFrame(productionDF))\n","    dictSubmission = {'fund_id':productionDFFundID, 'greatstone_rating':productionPrediction}\n","    dfSubmission = pd.DataFrame(dictSubmission)\n","    print('Saving File...')\n","    submissionPath = '/content/gdrive/My Drive/AIML/Hackathon_04112020/wd/Submissions/'+ datetime_object+'.csv'\n","    dfSubmission.to_csv(submissionPath, index = False)\n","    print('Submission File Saved in path')\n","  \n","  if True == False:\n","    print('Saving Model')\n","    modelName = '/content/gdrive/My Drive/AIML/Hackathon_04112020/wd/Model/'+strModelName+'_'+str(round(testPrecessionScore,2))+'_'+datetime_object+'.dat'\n","    pickle.dump(classifier, open(modelName, \"wb\"))\n","    print('Model Saved')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"t4MhwhvxqYSl"},"source":["# Other"]},{"cell_type":"code","metadata":{"id":"G9pS2yZC-iH3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}